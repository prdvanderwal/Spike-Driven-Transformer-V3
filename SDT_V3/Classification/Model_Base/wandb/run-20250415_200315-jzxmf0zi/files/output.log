/home4/p315895/venvs/lisnn/lib/python3.11/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[20:03:16.577301] Model = Spiking_vit_MetaFormer_Spike_SepConv(
  (downsample1_1): MS_DownSampling(
    (encode_conv): Conv2d(3, 16, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))
    (encode_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (ConvBlock1_1): ModuleList(
    (0): MS_ConvBlock_spike_SepConv(
      (Conv): SepConv_Spike(
        (spike1): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (pwconv1): Sequential(
          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (spike2): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (dwconv): Sequential(
          (0): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=32, bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (spike3): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (pwconv2): Sequential(
          (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (spike1): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
      (conv1): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (spike2): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
      (conv2): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (downsample1_2): MS_DownSampling(
    (encode_conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (encode_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (encode_spike): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
  )
  (ConvBlock1_2): ModuleList(
    (0): MS_ConvBlock_spike_SepConv(
      (Conv): SepConv_Spike(
        (spike1): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (pwconv1): Sequential(
          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (spike2): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (dwconv): Sequential(
          (0): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=64, bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (spike3): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (pwconv2): Sequential(
          (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (spike1): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
      (conv1): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (spike2): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (downsample2): MS_DownSampling(
    (encode_conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (encode_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (encode_spike): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
  )
  (ConvBlock2_1): ModuleList(
    (0): MS_ConvBlock_spike_SepConv(
      (Conv): SepConv_Spike(
        (spike1): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (pwconv1): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (spike2): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (dwconv): Sequential(
          (0): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (spike3): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (pwconv2): Sequential(
          (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (spike1): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
      (conv1): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (spike2): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
      (conv2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (ConvBlock2_2): ModuleList(
    (0): MS_ConvBlock_spike_SepConv(
      (Conv): SepConv_Spike(
        (spike1): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (pwconv1): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (spike2): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (dwconv): Sequential(
          (0): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (spike3): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (pwconv2): Sequential(
          (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (spike1): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
      (conv1): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (spike2): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
      (conv2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (downsample3): MS_DownSampling(
    (encode_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (encode_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (encode_spike): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
  )
  (block3): ModuleList(
    (0-5): 6 x MS_Block_Spike_SepConv(
      (conv): SepConv_Spike(
        (spike1): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (pwconv1): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (spike2): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (dwconv): Sequential(
          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (spike3): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (pwconv2): Sequential(
          (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (attn): MS_Attention_linear_with_LateralInhibition(
        (head_spike): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (qe_conv): Sequential(
          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (qe_spike): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (qi_conv): Sequential(
          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (qi_spike): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (k_conv): Sequential(
          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (k_spike): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (v_conv): Sequential(
          (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (v_spike): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (attn_spike): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (excitatory_spike): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (inhibitory_spike): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (combined_spike): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (proj_conv): Sequential(
          (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (drop_path): Identity()
      (mlp): MS_MLP(
        (fc1_conv): Conv1d(128, 512, kernel_size=(1,), stride=(1,))
        (fc1_bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (fc1_spike): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (fc2_conv): Conv1d(512, 128, kernel_size=(1,), stride=(1,))
        (fc2_bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (fc2_spike): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
      )
    )
  )
  (downsample4): MS_DownSampling(
    (encode_conv): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (encode_bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (encode_spike): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
  )
  (block4): ModuleList(
    (0-1): 2 x MS_Block_Spike_SepConv(
      (conv): SepConv_Spike(
        (spike1): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (pwconv1): Sequential(
          (0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (spike2): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (dwconv): Sequential(
          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (spike3): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (pwconv2): Sequential(
          (0): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (attn): MS_Attention_linear_with_LateralInhibition(
        (head_spike): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (qe_conv): Sequential(
          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (qe_spike): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (qi_conv): Sequential(
          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (qi_spike): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (k_conv): Sequential(
          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (k_spike): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (v_conv): Sequential(
          (0): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (v_spike): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (attn_spike): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (excitatory_spike): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (inhibitory_spike): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (combined_spike): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (proj_conv): Sequential(
          (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (drop_path): Identity()
      (mlp): MS_MLP(
        (fc1_conv): Conv1d(192, 768, kernel_size=(1,), stride=(1,))
        (fc1_bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (fc1_spike): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
        (fc2_conv): Conv1d(768, 192, kernel_size=(1,), stride=(1,))
        (fc2_bn): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (fc2_spike): MultiSpike(Max_Value=4, Min_Value=0, Norm=4)
      )
    )
  )
  (head): Linear(in_features=192, out_features=1000, bias=True)
  (spike): MultiSpike(Max_Value=4, Min_Value=0, Norm=1)
)
[20:03:16.577523] number of params (M): 5.28
[20:03:16.577648] base lr: 6.00e-04
[20:03:16.577734] actual lr: 6.00e-04
[20:03:16.577810] accumulate grad iterations: 1
[20:03:16.577886] effective batch size: 256
[20:03:16.595664] criterion = LabelSmoothingCrossEntropy()
[20:03:16.595809] Start training for 20 epochs
[20:03:23.970847] Epoch: [0]  [   0/1011]  eta: 2:04:13  lr: 0.000000  loss: 6.9212 (6.9212)  acc1: 0.0000 (0.0000)  acc5: 0.7812 (0.7812)  time: 7.3727  data: 2.9544  max mem: 19954
[20:04:05.993884] Epoch: [0]  [ 100/1011]  eta: 0:07:25  lr: 0.000012  loss: 6.9144 (6.9156)  acc1: 0.0000 (0.1083)  acc5: 0.3906 (0.4873)  time: 0.4192  data: 0.0004  max mem: 19954
[20:04:47.971271] Epoch: [0]  [ 200/1011]  eta: 0:06:08  lr: 0.000024  loss: 6.9077 (6.9134)  acc1: 0.0000 (0.0933)  acc5: 0.3906 (0.4936)  time: 0.4202  data: 0.0004  max mem: 19954
[20:05:30.068434] Epoch: [0]  [ 300/1011]  eta: 0:05:15  lr: 0.000036  loss: 6.8975 (6.9097)  acc1: 0.0000 (0.1116)  acc5: 0.7812 (0.5918)  time: 0.4209  data: 0.0004  max mem: 19954
[20:06:12.199892] Epoch: [0]  [ 400/1011]  eta: 0:04:27  lr: 0.000047  loss: 6.8855 (6.9049)  acc1: 0.0000 (0.1461)  acc5: 1.1719 (0.7257)  time: 0.4212  data: 0.0004  max mem: 19954
[20:06:54.416163] Epoch: [0]  [ 500/1011]  eta: 0:03:42  lr: 0.000059  loss: 6.8633 (6.8983)  acc1: 0.3906 (0.2051)  acc5: 1.9531 (0.9473)  time: 0.4214  data: 0.0005  max mem: 19954
[20:07:36.542913] Epoch: [0]  [ 600/1011]  eta: 0:02:57  lr: 0.000071  loss: 6.8268 (6.8892)  acc1: 0.3906 (0.2561)  acc5: 2.7344 (1.1953)  time: 0.4211  data: 0.0004  max mem: 19954
[20:08:18.666974] Epoch: [0]  [ 700/1011]  eta: 0:02:14  lr: 0.000083  loss: 6.7746 (6.8762)  acc1: 0.7812 (0.2898)  acc5: 2.7344 (1.3920)  time: 0.4209  data: 0.0004  max mem: 19954
[20:09:00.783719] Epoch: [0]  [ 800/1011]  eta: 0:01:30  lr: 0.000095  loss: 6.6978 (6.8579)  acc1: 0.7812 (0.3224)  acc5: 2.7344 (1.5581)  time: 0.4213  data: 0.0004  max mem: 19954
[20:09:42.881671] Epoch: [0]  [ 900/1011]  eta: 0:00:47  lr: 0.000107  loss: 6.6117 (6.8339)  acc1: 0.3906 (0.3473)  acc5: 2.7344 (1.6956)  time: 0.4210  data: 0.0004  max mem: 19954
[20:10:24.964252] Epoch: [0]  [1000/1011]  eta: 0:00:04  lr: 0.000119  loss: 6.5054 (6.8047)  acc1: 0.7812 (0.3820)  acc5: 3.5156 (1.8427)  time: 0.4202  data: 0.0004  max mem: 19954
[20:10:29.153758] Epoch: [0]  [1010/1011]  eta: 0:00:00  lr: 0.000120  loss: 6.4969 (6.8017)  acc1: 0.7812 (0.3871)  acc5: 3.1250 (1.8561)  time: 0.4192  data: 0.0003  max mem: 19954
[20:10:29.232429] Epoch: [0] Total time: 0:07:12 (0.4279 s / it)
[20:10:29.233898] Averaged stats: lr: 0.000120  loss: 6.4969 (6.8017)  acc1: 0.7812 (0.3871)  acc5: 3.1250 (1.8561)
[20:10:29.234482] * Train_Acc@1 0.387 Acc@5 1.856 loss 6.802
[20:10:29.235396] Saving model at epoch: 0
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[20:10:31.668654] Test:  [ 0/40]  eta: 0:01:25  loss: 6.4118 (6.4118)  acc1: 0.7812 (0.7812)  acc5: 3.5156 (3.5156)  time: 2.1430  data: 1.9535  max mem: 19954
[20:10:40.535553] Test:  [39/40]  eta: 0:00:00  loss: 6.3918 (6.3916)  acc1: 0.7812 (0.8000)  acc5: 4.2969 (3.9900)  time: 0.2430  data: 0.1039  max mem: 37267
[20:10:40.616696] Test: Total time: 0:00:11 (0.2773 s / it)
[20:10:40.618550] * Acc@1 0.800 Acc@5 3.990 loss 6.392
[20:10:40.619067] Accuracy of the network on the 10000 test images: 0.8%
[20:10:40.619342] Max accuracy: 0.80%
[20:10:40.619570] Saving model at epoch: 0
[20:10:43.387940] Epoch: [1]  [   0/1011]  eta: 0:41:40  lr: 0.000120  loss: 6.4775 (6.4775)  acc1: 0.7812 (0.7812)  acc5: 2.7344 (2.7344)  time: 2.4737  data: 1.9277  max mem: 37267
[20:11:25.910541] Epoch: [1]  [ 100/1011]  eta: 0:06:45  lr: 0.000132  loss: 6.3924 (6.4366)  acc1: 0.3906 (0.7116)  acc5: 3.1250 (3.1791)  time: 0.4205  data: 0.0004  max mem: 37267
[20:12:07.952880] Epoch: [1]  [ 200/1011]  eta: 0:05:51  lr: 0.000144  loss: 6.3015 (6.3914)  acc1: 0.3906 (0.7016)  acc5: 2.7344 (3.1328)  time: 0.4207  data: 0.0005  max mem: 37267
[20:12:50.027988] Epoch: [1]  [ 300/1011]  eta: 0:05:04  lr: 0.000156  loss: 6.2044 (6.3428)  acc1: 0.7812 (0.7397)  acc5: 3.1250 (3.1886)  time: 0.4206  data: 0.0004  max mem: 37267
[20:13:32.098351] Epoch: [1]  [ 400/1011]  eta: 0:04:20  lr: 0.000167  loss: 6.1127 (6.2935)  acc1: 0.7812 (0.7511)  acc5: 3.5156 (3.2468)  time: 0.4207  data: 0.0004  max mem: 37267
[20:14:14.146779] Epoch: [1]  [ 500/1011]  eta: 0:03:37  lr: 0.000179  loss: 6.0124 (6.2463)  acc1: 0.7812 (0.7727)  acc5: 3.5156 (3.2653)  time: 0.4204  data: 0.0004  max mem: 37267
[20:14:56.231260] Epoch: [1]  [ 600/1011]  eta: 0:02:54  lr: 0.000191  loss: 5.9327 (6.2008)  acc1: 0.7812 (0.8118)  acc5: 3.5156 (3.3115)  time: 0.4207  data: 0.0004  max mem: 37267
[20:15:38.422056] Epoch: [1]  [ 700/1011]  eta: 0:02:11  lr: 0.000203  loss: 5.8491 (6.1562)  acc1: 0.7812 (0.8281)  acc5: 4.6875 (3.4176)  time: 0.4210  data: 0.0004  max mem: 37267
[20:16:20.510794] Epoch: [1]  [ 800/1011]  eta: 0:01:29  lr: 0.000215  loss: 5.8032 (6.1142)  acc1: 1.1719 (0.8734)  acc5: 4.6875 (3.5463)  time: 0.4209  data: 0.0004  max mem: 37267
[20:17:02.604387] Epoch: [1]  [ 900/1011]  eta: 0:00:47  lr: 0.000227  loss: 5.7266 (6.0747)  acc1: 1.1719 (0.9326)  acc5: 4.6875 (3.7064)  time: 0.4210  data: 0.0004  max mem: 37267
[20:17:44.687004] Epoch: [1]  [1000/1011]  eta: 0:00:04  lr: 0.000239  loss: 5.7006 (6.0389)  acc1: 1.1719 (0.9647)  acc5: 5.4688 (3.8411)  time: 0.4204  data: 0.0005  max mem: 37267
[20:17:48.873583] Epoch: [1]  [1010/1011]  eta: 0:00:00  lr: 0.000240  loss: 5.7006 (6.0355)  acc1: 1.1719 (0.9648)  acc5: 5.0781 (3.8502)  time: 0.4191  data: 0.0003  max mem: 37267
[20:17:48.973278] Epoch: [1] Total time: 0:07:08 (0.4234 s / it)
[20:17:48.975007] Averaged stats: lr: 0.000240  loss: 5.7006 (6.0355)  acc1: 1.1719 (0.9648)  acc5: 5.0781 (3.8502)
[20:17:48.975760] * Train_Acc@1 0.965 Acc@5 3.850 loss 6.036
[20:17:50.948443] Test:  [ 0/40]  eta: 0:01:18  loss: 5.4145 (5.4145)  acc1: 1.5625 (1.5625)  acc5: 5.4688 (5.4688)  time: 1.9686  data: 1.8123  max mem: 37267
[20:17:58.953742] Test:  [39/40]  eta: 0:00:00  loss: 5.3766 (5.3788)  acc1: 1.5625 (1.6800)  acc5: 7.0312 (6.6300)  time: 0.1669  data: 0.0660  max mem: 37267
[20:17:59.041521] Test: Total time: 0:00:10 (0.2516 s / it)
[20:17:59.042799] * Acc@1 1.680 Acc@5 6.630 loss 5.379
[20:17:59.043107] Accuracy of the network on the 10000 test images: 1.7%
[20:17:59.043259] Max accuracy: 1.68%
[20:17:59.043387] Saving model at epoch: 1
[20:18:01.659427] Epoch: [2]  [   0/1011]  eta: 0:39:50  lr: 0.000240  loss: 5.6776 (5.6776)  acc1: 1.5625 (1.5625)  acc5: 7.4219 (7.4219)  time: 2.3646  data: 1.8705  max mem: 37267
[20:18:44.927727] Epoch: [2]  [ 100/1011]  eta: 0:06:51  lr: 0.000252  loss: 5.6552 (5.6695)  acc1: 1.1719 (1.5200)  acc5: 5.8594 (5.7782)  time: 0.4211  data: 0.0004  max mem: 37267
[20:19:27.023975] Epoch: [2]  [ 200/1011]  eta: 0:05:53  lr: 0.000264  loss: 5.6356 (5.6541)  acc1: 1.9531 (1.5664)  acc5: 6.2500 (6.0809)  time: 0.4207  data: 0.0005  max mem: 37267
[20:20:09.120276] Epoch: [2]  [ 300/1011]  eta: 0:05:06  lr: 0.000276  loss: 5.6044 (5.6419)  acc1: 1.5625 (1.6650)  acc5: 6.2500 (6.2318)  time: 0.4210  data: 0.0005  max mem: 37267
[20:20:51.226592] Epoch: [2]  [ 400/1011]  eta: 0:04:21  lr: 0.000287  loss: 5.5808 (5.6302)  acc1: 1.9531 (1.7271)  acc5: 7.8125 (6.6065)  time: 0.4211  data: 0.0005  max mem: 37267
[20:21:33.317667] Epoch: [2]  [ 500/1011]  eta: 0:03:38  lr: 0.000299  loss: 5.5681 (5.6188)  acc1: 2.3438 (1.7863)  acc5: 8.2031 (6.8948)  time: 0.4207  data: 0.0005  max mem: 37267
[20:22:15.405419] Epoch: [2]  [ 600/1011]  eta: 0:02:55  lr: 0.000311  loss: 5.5530 (5.6076)  acc1: 1.9531 (1.8602)  acc5: 8.2031 (7.1879)  time: 0.4210  data: 0.0004  max mem: 37267
[20:22:57.500851] Epoch: [2]  [ 700/1011]  eta: 0:02:12  lr: 0.000323  loss: 5.5315 (5.5970)  acc1: 2.3438 (1.9292)  acc5: 8.9844 (7.4637)  time: 0.4208  data: 0.0004  max mem: 37267
[20:23:39.602506] Epoch: [2]  [ 800/1011]  eta: 0:01:29  lr: 0.000335  loss: 5.5051 (5.5855)  acc1: 2.7344 (2.0370)  acc5: 9.3750 (7.7969)  time: 0.4210  data: 0.0004  max mem: 37267
[20:24:21.698332] Epoch: [2]  [ 900/1011]  eta: 0:00:47  lr: 0.000347  loss: 5.4797 (5.5749)  acc1: 2.3438 (2.1092)  acc5: 10.1562 (8.0401)  time: 0.4209  data: 0.0004  max mem: 37267
[20:25:03.878227] Epoch: [2]  [1000/1011]  eta: 0:00:04  lr: 0.000359  loss: 5.4501 (5.5637)  acc1: 3.1250 (2.1990)  acc5: 11.3281 (8.3331)  time: 0.4204  data: 0.0005  max mem: 37267
[20:25:08.070834] Epoch: [2]  [1010/1011]  eta: 0:00:00  lr: 0.000360  loss: 5.4485 (5.5626)  acc1: 3.1250 (2.2047)  acc5: 11.3281 (8.3557)  time: 0.4195  data: 0.0003  max mem: 37267
[20:25:08.165821] Epoch: [2] Total time: 0:07:08 (0.4242 s / it)
[20:25:08.167250] Averaged stats: lr: 0.000360  loss: 5.4485 (5.5626)  acc1: 3.1250 (2.2047)  acc5: 11.3281 (8.3557)
[20:25:08.167858] * Train_Acc@1 2.205 Acc@5 8.356 loss 5.563
[20:25:11.826009] Test:  [ 0/40]  eta: 0:02:26  loss: 4.9911 (4.9911)  acc1: 2.3438 (2.3438)  acc5: 13.2812 (13.2812)  time: 3.6546  data: 3.5085  max mem: 37267
[20:25:18.222958] Test:  [39/40]  eta: 0:00:00  loss: 4.9465 (4.9471)  acc1: 4.6875 (4.5400)  acc5: 15.6250 (15.5600)  time: 0.1817  data: 0.0808  max mem: 37267
[20:25:18.307338] Test: Total time: 0:00:10 (0.2534 s / it)
[20:25:18.310302] * Acc@1 4.540 Acc@5 15.560 loss 4.947
[20:25:18.311065] Accuracy of the network on the 10000 test images: 4.5%
[20:25:18.311521] Max accuracy: 4.54%
[20:25:18.311933] Saving model at epoch: 2
[20:25:21.056439] Epoch: [3]  [   0/1011]  eta: 0:41:36  lr: 0.000360  loss: 5.4117 (5.4117)  acc1: 3.5156 (3.5156)  acc5: 11.3281 (11.3281)  time: 2.4693  data: 1.9608  max mem: 37267
[20:26:03.738908] Epoch: [3]  [ 100/1011]  eta: 0:06:47  lr: 0.000372  loss: 5.4256 (5.4387)  acc1: 3.1250 (3.4537)  acc5: 11.7188 (11.6259)  time: 0.4206  data: 0.0004  max mem: 37267
[20:26:45.815385] Epoch: [3]  [ 200/1011]  eta: 0:05:51  lr: 0.000384  loss: 5.3728 (5.4236)  acc1: 3.5156 (3.5001)  acc5: 12.5000 (11.9578)  time: 0.4208  data: 0.0004  max mem: 37267
[20:27:27.908908] Epoch: [3]  [ 300/1011]  eta: 0:05:05  lr: 0.000396  loss: 5.3845 (5.4117)  acc1: 3.1250 (3.5416)  acc5: 12.8906 (12.3274)  time: 0.4208  data: 0.0004  max mem: 37267
[20:28:09.985933] Epoch: [3]  [ 400/1011]  eta: 0:04:21  lr: 0.000407  loss: 5.3461 (5.3985)  acc1: 3.5156 (3.6189)  acc5: 14.0625 (12.6471)  time: 0.4207  data: 0.0004  max mem: 37267
[20:28:52.076016] Epoch: [3]  [ 500/1011]  eta: 0:03:37  lr: 0.000419  loss: 5.3462 (5.3870)  acc1: 3.9062 (3.7293)  acc5: 13.6719 (12.9787)  time: 0.4209  data: 0.0004  max mem: 37267
[20:29:34.160798] Epoch: [3]  [ 600/1011]  eta: 0:02:54  lr: 0.000431  loss: 5.3033 (5.3739)  acc1: 4.2969 (3.8328)  acc5: 14.8438 (13.3183)  time: 0.4208  data: 0.0004  max mem: 37267
[20:30:16.234102] Epoch: [3]  [ 700/1011]  eta: 0:02:12  lr: 0.000443  loss: 5.2690 (5.3621)  acc1: 4.6875 (3.9453)  acc5: 15.2344 (13.6011)  time: 0.4208  data: 0.0004  max mem: 37267
[20:30:58.326593] Epoch: [3]  [ 800/1011]  eta: 0:01:29  lr: 0.000455  loss: 5.2502 (5.3493)  acc1: 5.0781 (4.0799)  acc5: 16.7969 (13.9459)  time: 0.4209  data: 0.0004  max mem: 37267
[20:31:40.432178] Epoch: [3]  [ 900/1011]  eta: 0:00:47  lr: 0.000467  loss: 5.2158 (5.3356)  acc1: 5.4688 (4.2067)  acc5: 17.1875 (14.2611)  time: 0.4210  data: 0.0004  max mem: 37267
[20:32:22.515110] Epoch: [3]  [1000/1011]  eta: 0:00:04  lr: 0.000479  loss: 5.2052 (5.3226)  acc1: 5.8594 (4.3281)  acc5: 16.7969 (14.5737)  time: 0.4202  data: 0.0005  max mem: 37267
[20:32:26.702775] Epoch: [3]  [1010/1011]  eta: 0:00:00  lr: 0.000480  loss: 5.2079 (5.3216)  acc1: 5.8594 (4.3320)  acc5: 16.7969 (14.5934)  time: 0.4192  data: 0.0003  max mem: 37267
[20:32:26.792758] Epoch: [3] Total time: 0:07:08 (0.4236 s / it)
[20:32:26.794084] Averaged stats: lr: 0.000480  loss: 5.2079 (5.3216)  acc1: 5.8594 (4.3320)  acc5: 16.7969 (14.5934)
[20:32:26.794669] * Train_Acc@1 4.332 Acc@5 14.593 loss 5.322
[20:32:30.409507] Test:  [ 0/40]  eta: 0:02:24  loss: 4.6042 (4.6042)  acc1: 7.4219 (7.4219)  acc5: 25.0000 (25.0000)  time: 3.6113  data: 3.4668  max mem: 37267
[20:32:36.533608] Test:  [39/40]  eta: 0:00:00  loss: 4.5335 (4.5413)  acc1: 7.4219 (8.0200)  acc5: 25.0000 (24.9200)  time: 0.1636  data: 0.0627  max mem: 37267
[20:32:36.610628] Test: Total time: 0:00:09 (0.2453 s / it)
[20:32:36.612469] * Acc@1 8.020 Acc@5 24.920 loss 4.541
[20:32:36.612969] Accuracy of the network on the 10000 test images: 8.0%
[20:32:36.613234] Max accuracy: 8.02%
[20:32:36.613450] Saving model at epoch: 3
[20:32:39.151387] Epoch: [4]  [   0/1011]  eta: 0:38:33  lr: 0.000480  loss: 5.1284 (5.1284)  acc1: 3.5156 (3.5156)  acc5: 21.8750 (21.8750)  time: 2.2888  data: 1.8096  max mem: 37267
[20:33:22.021606] Epoch: [4]  [ 100/1011]  eta: 0:06:47  lr: 0.000492  loss: 5.1791 (5.1727)  acc1: 5.0781 (5.6505)  acc5: 17.9688 (18.7809)  time: 0.4206  data: 0.0004  max mem: 37267
[20:34:04.195598] Epoch: [4]  [ 200/1011]  eta: 0:05:52  lr: 0.000504  loss: 5.1260 (5.1639)  acc1: 6.2500 (5.7758)  acc5: 18.7500 (18.8685)  time: 0.4209  data: 0.0005  max mem: 37267
[20:34:46.274991] Epoch: [4]  [ 300/1011]  eta: 0:05:05  lr: 0.000516  loss: 5.1150 (5.1499)  acc1: 6.2500 (5.9866)  acc5: 18.7500 (19.1575)  time: 0.4209  data: 0.0004  max mem: 37267
[20:35:28.364288] Epoch: [4]  [ 400/1011]  eta: 0:04:21  lr: 0.000527  loss: 5.0866 (5.1370)  acc1: 6.2500 (6.0659)  acc5: 20.3125 (19.4066)  time: 0.4209  data: 0.0004  max mem: 37267
[20:36:10.454803] Epoch: [4]  [ 500/1011]  eta: 0:03:37  lr: 0.000539  loss: 5.0755 (5.1244)  acc1: 6.6406 (6.2274)  acc5: 20.3125 (19.6685)  time: 0.4209  data: 0.0005  max mem: 37267
[20:36:52.563094] Epoch: [4]  [ 600/1011]  eta: 0:02:54  lr: 0.000551  loss: 5.0418 (5.1122)  acc1: 7.4219 (6.4209)  acc5: 21.4844 (20.0044)  time: 0.4211  data: 0.0004  max mem: 37267
[20:37:34.671142] Epoch: [4]  [ 700/1011]  eta: 0:02:12  lr: 0.000563  loss: 5.0143 (5.0987)  acc1: 7.8125 (6.5827)  acc5: 21.8750 (20.3264)  time: 0.4209  data: 0.0004  max mem: 37267
[20:38:16.783457] Epoch: [4]  [ 800/1011]  eta: 0:01:29  lr: 0.000575  loss: 4.9644 (5.0853)  acc1: 8.5938 (6.7543)  acc5: 23.0469 (20.6641)  time: 0.4210  data: 0.0004  max mem: 37267
[20:38:58.885521] Epoch: [4]  [ 900/1011]  eta: 0:00:47  lr: 0.000587  loss: 4.9362 (5.0718)  acc1: 8.2031 (6.9359)  acc5: 24.2188 (21.0357)  time: 0.4209  data: 0.0004  max mem: 37267
[20:39:40.967594] Epoch: [4]  [1000/1011]  eta: 0:00:04  lr: 0.000599  loss: 4.9522 (5.0597)  acc1: 7.8125 (7.0629)  acc5: 24.6094 (21.3486)  time: 0.4203  data: 0.0004  max mem: 37267
[20:39:45.158162] Epoch: [4]  [1010/1011]  eta: 0:00:00  lr: 0.000600  loss: 4.9522 (5.0585)  acc1: 8.2031 (7.0857)  acc5: 24.6094 (21.3808)  time: 0.4194  data: 0.0003  max mem: 37267
[20:39:45.259842] Epoch: [4] Total time: 0:07:08 (0.4237 s / it)
[20:39:45.262914] Averaged stats: lr: 0.000600  loss: 4.9522 (5.0585)  acc1: 8.2031 (7.0857)  acc5: 24.6094 (21.3808)
[20:39:45.264343] * Train_Acc@1 7.086 Acc@5 21.381 loss 5.058
[20:39:47.282491] Test:  [ 0/40]  eta: 0:01:20  loss: 4.3882 (4.3882)  acc1: 8.9844 (8.9844)  acc5: 26.1719 (26.1719)  time: 2.0132  data: 1.8553  max mem: 37267
[20:39:54.653668] Test:  [39/40]  eta: 0:00:00  loss: 4.3185 (4.3398)  acc1: 10.5469 (10.6000)  acc5: 28.5156 (28.7700)  time: 0.1318  data: 0.0306  max mem: 37267
[20:39:54.736684] Test: Total time: 0:00:09 (0.2367 s / it)
[20:39:54.738760] * Acc@1 10.600 Acc@5 28.770 loss 4.340
[20:39:54.739366] Accuracy of the network on the 10000 test images: 10.6%
[20:39:54.739710] Max accuracy: 10.60%
[20:39:54.739991] Saving model at epoch: 4
[20:39:58.589154] Epoch: [5]  [   0/1011]  eta: 0:59:21  lr: 0.000600  loss: 5.0418 (5.0418)  acc1: 8.2031 (8.2031)  acc5: 25.3906 (25.3906)  time: 3.5228  data: 3.0305  max mem: 37267
[20:40:40.649212] Epoch: [5]  [ 100/1011]  eta: 0:06:51  lr: 0.000600  loss: 4.8640 (4.9176)  acc1: 9.3750 (8.9264)  acc5: 25.3906 (25.1779)  time: 0.4205  data: 0.0004  max mem: 37267
[20:41:22.726992] Epoch: [5]  [ 200/1011]  eta: 0:05:53  lr: 0.000600  loss: 4.8918 (4.9084)  acc1: 9.3750 (8.9338)  acc5: 25.0000 (25.3518)  time: 0.4207  data: 0.0004  max mem: 37267
[20:42:04.815526] Epoch: [5]  [ 300/1011]  eta: 0:05:06  lr: 0.000599  loss: 4.8571 (4.8968)  acc1: 9.3750 (9.0765)  acc5: 26.5625 (25.6112)  time: 0.4209  data: 0.0005  max mem: 37267
[20:42:46.893043] Epoch: [5]  [ 400/1011]  eta: 0:04:21  lr: 0.000599  loss: 4.8721 (4.8870)  acc1: 8.9844 (9.2104)  acc5: 25.7812 (25.8533)  time: 0.4207  data: 0.0005  max mem: 37267
[20:43:29.097974] Epoch: [5]  [ 500/1011]  eta: 0:03:38  lr: 0.000598  loss: 4.8045 (4.8756)  acc1: 10.5469 (9.4187)  acc5: 27.7344 (26.1680)  time: 0.4215  data: 0.0004  max mem: 37267
[20:44:11.188183] Epoch: [5]  [ 600/1011]  eta: 0:02:55  lr: 0.000598  loss: 4.8158 (4.8646)  acc1: 9.7656 (9.5570)  acc5: 28.1250 (26.4345)  time: 0.4209  data: 0.0005  max mem: 37267
[20:44:53.297835] Epoch: [5]  [ 700/1011]  eta: 0:02:12  lr: 0.000597  loss: 4.7807 (4.8529)  acc1: 10.1562 (9.7433)  acc5: 27.7344 (26.7921)  time: 0.4215  data: 0.0005  max mem: 37267
[20:45:35.408319] Epoch: [5]  [ 800/1011]  eta: 0:01:29  lr: 0.000596  loss: 4.7586 (4.8437)  acc1: 10.9375 (9.8510)  acc5: 28.5156 (27.0594)  time: 0.4212  data: 0.0005  max mem: 37267
[20:46:17.520127] Epoch: [5]  [ 900/1011]  eta: 0:00:47  lr: 0.000595  loss: 4.7253 (4.8323)  acc1: 11.7188 (10.0557)  acc5: 29.2969 (27.3715)  time: 0.4210  data: 0.0004  max mem: 37267
[20:46:59.605067] Epoch: [5]  [1000/1011]  eta: 0:00:04  lr: 0.000594  loss: 4.7315 (4.8195)  acc1: 12.1094 (10.2659)  acc5: 30.4688 (27.7110)  time: 0.4203  data: 0.0005  max mem: 37267
[20:47:03.793730] Epoch: [5]  [1010/1011]  eta: 0:00:00  lr: 0.000593  loss: 4.7157 (4.8186)  acc1: 11.3281 (10.2756)  acc5: 30.4688 (27.7390)  time: 0.4191  data: 0.0003  max mem: 37267
[20:47:03.899267] Epoch: [5] Total time: 0:07:08 (0.4242 s / it)
[20:47:03.902769] Averaged stats: lr: 0.000593  loss: 4.7157 (4.8186)  acc1: 11.3281 (10.2756)  acc5: 30.4688 (27.7390)
[20:47:03.904387] * Train_Acc@1 10.276 Acc@5 27.739 loss 4.819
[20:47:06.648511] Test:  [ 0/40]  eta: 0:01:49  loss: 3.8522 (3.8522)  acc1: 19.5312 (19.5312)  acc5: 41.0156 (41.0156)  time: 2.7388  data: 2.5967  max mem: 37267
[20:47:13.347871] Test:  [39/40]  eta: 0:00:00  loss: 3.8380 (3.8420)  acc1: 17.5781 (17.5300)  acc5: 40.6250 (40.8400)  time: 0.1499  data: 0.0491  max mem: 37267
[20:47:13.426253] Test: Total time: 0:00:09 (0.2379 s / it)
[20:47:13.427525] * Acc@1 17.530 Acc@5 40.840 loss 3.842
[20:47:13.427834] Accuracy of the network on the 10000 test images: 17.5%
[20:47:13.427982] Max accuracy: 17.53%
[20:47:13.428103] Saving model at epoch: 5
[20:47:16.378377] Epoch: [6]  [   0/1011]  eta: 0:44:45  lr: 0.000593  loss: 4.7774 (4.7774)  acc1: 11.3281 (11.3281)  acc5: 27.7344 (27.7344)  time: 2.6567  data: 2.1392  max mem: 37267
[20:47:58.590586] Epoch: [6]  [ 100/1011]  eta: 0:06:44  lr: 0.000592  loss: 4.6932 (4.7038)  acc1: 12.1094 (12.0398)  acc5: 31.6406 (30.9213)  time: 0.4205  data: 0.0005  max mem: 37267
[20:48:40.634844] Epoch: [6]  [ 200/1011]  eta: 0:05:50  lr: 0.000591  loss: 4.6471 (4.6907)  acc1: 12.1094 (12.2279)  acc5: 31.6406 (31.4035)  time: 0.4206  data: 0.0004  max mem: 37267
[20:49:22.722308] Epoch: [6]  [ 300/1011]  eta: 0:05:04  lr: 0.000589  loss: 4.6371 (4.6770)  acc1: 11.7188 (12.4507)  acc5: 32.0312 (31.6536)  time: 0.4207  data: 0.0004  max mem: 37267
[20:50:04.813679] Epoch: [6]  [ 400/1011]  eta: 0:04:20  lr: 0.000587  loss: 4.6404 (4.6668)  acc1: 12.5000 (12.5955)  acc5: 32.4219 (31.9377)  time: 0.4208  data: 0.0005  max mem: 37267
[20:50:46.929212] Epoch: [6]  [ 500/1011]  eta: 0:03:37  lr: 0.000585  loss: 4.6346 (4.6595)  acc1: 12.8906 (12.7495)  acc5: 32.0312 (32.1014)  time: 0.4210  data: 0.0004  max mem: 37267
[20:51:29.040947] Epoch: [6]  [ 600/1011]  eta: 0:02:54  lr: 0.000583  loss: 4.6086 (4.6473)  acc1: 13.6719 (12.9407)  acc5: 32.8125 (32.3861)  time: 0.4211  data: 0.0004  max mem: 37267
[20:52:11.213755] Epoch: [6]  [ 700/1011]  eta: 0:02:11  lr: 0.000581  loss: 4.5723 (4.6388)  acc1: 13.6719 (13.0829)  acc5: 34.3750 (32.5946)  time: 0.4245  data: 0.0004  max mem: 37267
[20:52:53.320295] Epoch: [6]  [ 800/1011]  eta: 0:01:29  lr: 0.000579  loss: 4.5701 (4.6302)  acc1: 14.0625 (13.2515)  acc5: 34.7656 (32.8461)  time: 0.4211  data: 0.0004  max mem: 37267
[20:53:35.427046] Epoch: [6]  [ 900/1011]  eta: 0:00:47  lr: 0.000577  loss: 4.5507 (4.6203)  acc1: 15.2344 (13.4113)  acc5: 35.5469 (33.0874)  time: 0.4209  data: 0.0004  max mem: 37267
[20:54:17.514564] Epoch: [6]  [1000/1011]  eta: 0:00:04  lr: 0.000574  loss: 4.5159 (4.6117)  acc1: 14.4531 (13.5357)  acc5: 36.3281 (33.3257)  time: 0.4201  data: 0.0004  max mem: 37267
[20:54:21.700829] Epoch: [6]  [1010/1011]  eta: 0:00:00  lr: 0.000574  loss: 4.5032 (4.6105)  acc1: 14.8438 (13.5502)  acc5: 36.3281 (33.3557)  time: 0.4190  data: 0.0003  max mem: 37267
[20:54:21.799309] Epoch: [6] Total time: 0:07:08 (0.4234 s / it)
[20:54:21.802770] Averaged stats: lr: 0.000574  loss: 4.5032 (4.6105)  acc1: 14.8438 (13.5502)  acc5: 36.3281 (33.3557)
[20:54:21.804269] * Train_Acc@1 13.550 Acc@5 33.356 loss 4.611
[20:54:23.771535] Test:  [ 0/40]  eta: 0:01:18  loss: 3.6363 (3.6363)  acc1: 20.7031 (20.7031)  acc5: 42.5781 (42.5781)  time: 1.9623  data: 1.8053  max mem: 37267
[20:54:30.943761] Test:  [39/40]  eta: 0:00:00  loss: 3.5539 (3.6072)  acc1: 21.8750 (21.1600)  acc5: 47.6562 (46.1200)  time: 0.1512  data: 0.0479  max mem: 37267
[20:54:31.029484] Test: Total time: 0:00:09 (0.2305 s / it)
[20:54:31.030716] * Acc@1 21.160 Acc@5 46.120 loss 3.607
[20:54:31.031021] Accuracy of the network on the 10000 test images: 21.2%
[20:54:31.031172] Max accuracy: 21.16%
[20:54:31.031295] Saving model at epoch: 6
[20:54:33.600844] Epoch: [7]  [   0/1011]  eta: 0:38:27  lr: 0.000574  loss: 4.4059 (4.4059)  acc1: 16.7969 (16.7969)  acc5: 41.0156 (41.0156)  time: 2.2823  data: 1.7671  max mem: 37267
[20:55:16.998513] Epoch: [7]  [ 100/1011]  eta: 0:06:52  lr: 0.000572  loss: 4.4891 (4.4895)  acc1: 15.6250 (15.7642)  acc5: 36.3281 (36.7806)  time: 0.4209  data: 0.0004  max mem: 37267
[20:55:59.085161] Epoch: [7]  [ 200/1011]  eta: 0:05:54  lr: 0.000569  loss: 4.4956 (4.4941)  acc1: 16.0156 (15.5842)  acc5: 36.7188 (36.5361)  time: 0.4207  data: 0.0005  max mem: 37267
[20:56:41.183245] Epoch: [7]  [ 300/1011]  eta: 0:05:06  lr: 0.000566  loss: 4.4710 (4.4901)  acc1: 16.0156 (15.6523)  acc5: 35.5469 (36.5864)  time: 0.4210  data: 0.0005  max mem: 37267
[20:57:23.312795] Epoch: [7]  [ 400/1011]  eta: 0:04:22  lr: 0.000563  loss: 4.4393 (4.4834)  acc1: 16.0156 (15.6883)  acc5: 37.5000 (36.8434)  time: 0.4214  data: 0.0004  max mem: 37267
[20:58:05.407960] Epoch: [7]  [ 500/1011]  eta: 0:03:38  lr: 0.000560  loss: 4.4381 (4.4753)  acc1: 16.7969 (15.8308)  acc5: 36.7188 (37.0665)  time: 0.4207  data: 0.0004  max mem: 37267
[20:58:47.506083] Epoch: [7]  [ 600/1011]  eta: 0:02:55  lr: 0.000557  loss: 4.4432 (4.4703)  acc1: 17.1875 (15.9623)  acc5: 37.5000 (37.2186)  time: 0.4209  data: 0.0005  max mem: 37267
[20:59:29.602605] Epoch: [7]  [ 700/1011]  eta: 0:02:12  lr: 0.000554  loss: 4.4394 (4.4623)  acc1: 16.4062 (16.1143)  acc5: 37.8906 (37.4671)  time: 0.4210  data: 0.0004  max mem: 37267
[21:00:11.722298] Epoch: [7]  [ 800/1011]  eta: 0:01:29  lr: 0.000550  loss: 4.3815 (4.4537)  acc1: 16.7969 (16.2873)  acc5: 39.4531 (37.7068)  time: 0.4214  data: 0.0004  max mem: 37267
[21:00:53.812850] Epoch: [7]  [ 900/1011]  eta: 0:00:47  lr: 0.000547  loss: 4.3750 (4.4454)  acc1: 17.1875 (16.4210)  acc5: 39.4531 (37.9127)  time: 0.4209  data: 0.0004  max mem: 37267
[21:01:35.975372] Epoch: [7]  [1000/1011]  eta: 0:00:04  lr: 0.000543  loss: 4.3632 (4.4379)  acc1: 17.5781 (16.5647)  acc5: 39.0625 (38.1088)  time: 0.4202  data: 0.0005  max mem: 37267
[21:01:40.164843] Epoch: [7]  [1010/1011]  eta: 0:00:00  lr: 0.000543  loss: 4.3595 (4.4369)  acc1: 17.1875 (16.5848)  acc5: 39.4531 (38.1344)  time: 0.4193  data: 0.0003  max mem: 37267
[21:01:40.268926] Epoch: [7] Total time: 0:07:08 (0.4243 s / it)
[21:01:40.272298] Averaged stats: lr: 0.000543  loss: 4.3595 (4.4369)  acc1: 17.1875 (16.5848)  acc5: 39.4531 (38.1344)
[21:01:40.273891] * Train_Acc@1 16.585 Acc@5 38.134 loss 4.437
[21:01:43.017017] Test:  [ 0/40]  eta: 0:01:49  loss: 3.4253 (3.4253)  acc1: 26.9531 (26.9531)  acc5: 48.8281 (48.8281)  time: 2.7380  data: 2.5899  max mem: 37267
[21:01:49.804603] Test:  [39/40]  eta: 0:00:00  loss: 3.3649 (3.3884)  acc1: 25.3906 (25.0100)  acc5: 51.5625 (51.0000)  time: 0.1694  data: 0.0684  max mem: 37267
[21:01:49.891585] Test: Total time: 0:00:09 (0.2403 s / it)
[21:01:49.894536] * Acc@1 25.010 Acc@5 51.000 loss 3.388
[21:01:49.895315] Accuracy of the network on the 10000 test images: 25.0%
[21:01:49.895812] Max accuracy: 25.01%
[21:01:49.896220] Saving model at epoch: 7
[21:01:54.085201] Epoch: [8]  [   0/1011]  eta: 1:05:26  lr: 0.000543  loss: 4.3518 (4.3518)  acc1: 16.4062 (16.4062)  acc5: 37.1094 (37.1094)  time: 3.8842  data: 3.3960  max mem: 37267
[21:02:36.176956] Epoch: [8]  [ 100/1011]  eta: 0:06:54  lr: 0.000539  loss: 4.3443 (4.3488)  acc1: 18.3594 (17.7986)  acc5: 40.6250 (40.3736)  time: 0.4212  data: 0.0004  max mem: 37267
[21:03:18.292682] Epoch: [8]  [ 200/1011]  eta: 0:05:55  lr: 0.000535  loss: 4.3606 (4.3364)  acc1: 18.3594 (18.1203)  acc5: 40.2344 (40.7435)  time: 0.4210  data: 0.0004  max mem: 37267
[21:04:00.391559] Epoch: [8]  [ 300/1011]  eta: 0:05:07  lr: 0.000531  loss: 4.2984 (4.3327)  acc1: 18.3594 (18.2296)  acc5: 41.0156 (40.8534)  time: 0.4212  data: 0.0005  max mem: 37267
[21:04:42.505438] Epoch: [8]  [ 400/1011]  eta: 0:04:22  lr: 0.000527  loss: 4.2909 (4.3258)  acc1: 19.5312 (18.3292)  acc5: 41.7969 (40.9747)  time: 0.4209  data: 0.0004  max mem: 37267
[21:05:24.649781] Epoch: [8]  [ 500/1011]  eta: 0:03:38  lr: 0.000523  loss: 4.2967 (4.3208)  acc1: 19.1406 (18.4919)  acc5: 41.4062 (41.1302)  time: 0.4212  data: 0.0004  max mem: 37267
[21:06:06.777087] Epoch: [8]  [ 600/1011]  eta: 0:02:55  lr: 0.000519  loss: 4.2630 (4.3129)  acc1: 19.5312 (18.6395)  acc5: 41.7969 (41.3413)  time: 0.4213  data: 0.0004  max mem: 37267
[21:06:48.894492] Epoch: [8]  [ 700/1011]  eta: 0:02:12  lr: 0.000515  loss: 4.2676 (4.3095)  acc1: 18.7500 (18.7372)  acc5: 41.0156 (41.4597)  time: 0.4210  data: 0.0004  max mem: 37267
[21:07:31.029102] Epoch: [8]  [ 800/1011]  eta: 0:01:29  lr: 0.000510  loss: 4.2532 (4.3028)  acc1: 18.7500 (18.8612)  acc5: 41.7969 (41.6428)  time: 0.4213  data: 0.0004  max mem: 37267
[21:08:13.153658] Epoch: [8]  [ 900/1011]  eta: 0:00:47  lr: 0.000506  loss: 4.2438 (4.2959)  acc1: 20.3125 (19.0015)  acc5: 43.3594 (41.8181)  time: 0.4211  data: 0.0004  max mem: 37267
[21:08:55.255931] Epoch: [8]  [1000/1011]  eta: 0:00:04  lr: 0.000501  loss: 4.2009 (4.2896)  acc1: 19.9219 (19.0805)  acc5: 42.9688 (41.9791)  time: 0.4204  data: 0.0005  max mem: 37267
[21:08:59.438117] Epoch: [8]  [1010/1011]  eta: 0:00:00  lr: 0.000501  loss: 4.1909 (4.2888)  acc1: 19.9219 (19.0946)  acc5: 43.3594 (42.0044)  time: 0.4187  data: 0.0003  max mem: 37267
[21:08:59.532061] Epoch: [8] Total time: 0:07:09 (0.4247 s / it)
[21:08:59.534242] Averaged stats: lr: 0.000501  loss: 4.1909 (4.2888)  acc1: 19.9219 (19.0946)  acc5: 43.3594 (42.0044)
[21:08:59.535404] * Train_Acc@1 19.095 Acc@5 42.004 loss 4.289
[21:09:03.148244] Test:  [ 0/40]  eta: 0:02:24  loss: 3.1242 (3.1242)  acc1: 26.1719 (26.1719)  acc5: 56.2500 (56.2500)  time: 3.6067  data: 3.4608  max mem: 37267
[21:09:09.123539] Test:  [39/40]  eta: 0:00:00  loss: 3.0958 (3.1155)  acc1: 29.6875 (29.6400)  acc5: 57.0312 (57.5400)  time: 0.1802  data: 0.0787  max mem: 37267
[21:09:09.206466] Test: Total time: 0:00:09 (0.2416 s / it)
[21:09:09.208653] * Acc@1 29.640 Acc@5 57.540 loss 3.116
[21:09:09.209285] Accuracy of the network on the 10000 test images: 29.6%
[21:09:09.209660] Max accuracy: 29.64%
[21:09:09.209981] Saving model at epoch: 8
[21:09:13.486869] Epoch: [9]  [   0/1011]  eta: 1:06:53  lr: 0.000501  loss: 4.0255 (4.0255)  acc1: 21.8750 (21.8750)  acc5: 47.6562 (47.6562)  time: 3.9700  data: 3.4946  max mem: 37267
[21:09:55.555512] Epoch: [9]  [ 100/1011]  eta: 0:06:55  lr: 0.000496  loss: 4.2067 (4.2089)  acc1: 21.0938 (20.7302)  acc5: 43.3594 (44.0207)  time: 0.4207  data: 0.0005  max mem: 37267
[21:10:37.741103] Epoch: [9]  [ 200/1011]  eta: 0:05:55  lr: 0.000492  loss: 4.2236 (4.2166)  acc1: 19.9219 (20.5321)  acc5: 42.9688 (43.9968)  time: 0.4208  data: 0.0004  max mem: 37267
[21:11:19.836638] Epoch: [9]  [ 300/1011]  eta: 0:05:07  lr: 0.000487  loss: 4.1308 (4.2081)  acc1: 21.4844 (20.6538)  acc5: 44.9219 (44.1679)  time: 0.4209  data: 0.0004  max mem: 37267
[21:12:01.921367] Epoch: [9]  [ 400/1011]  eta: 0:04:22  lr: 0.000482  loss: 4.1737 (4.2003)  acc1: 21.4844 (20.8648)  acc5: 44.9219 (44.3238)  time: 0.4207  data: 0.0004  max mem: 37267
[21:12:44.010692] Epoch: [9]  [ 500/1011]  eta: 0:03:38  lr: 0.000477  loss: 4.2020 (4.1954)  acc1: 21.0938 (20.9643)  acc5: 44.9219 (44.5016)  time: 0.4208  data: 0.0005  max mem: 37267
[21:13:26.080048] Epoch: [9]  [ 600/1011]  eta: 0:02:55  lr: 0.000472  loss: 4.1454 (4.1889)  acc1: 21.4844 (21.0639)  acc5: 45.7031 (44.6619)  time: 0.4205  data: 0.0004  max mem: 37267
[21:14:08.151093] Epoch: [9]  [ 700/1011]  eta: 0:02:12  lr: 0.000467  loss: 4.1720 (4.1830)  acc1: 20.7031 (21.1545)  acc5: 45.3125 (44.7826)  time: 0.4209  data: 0.0005  max mem: 37267
[21:14:50.218560] Epoch: [9]  [ 800/1011]  eta: 0:01:29  lr: 0.000461  loss: 4.0690 (4.1780)  acc1: 23.8281 (21.2293)  acc5: 47.2656 (44.9209)  time: 0.4205  data: 0.0004  max mem: 37267
[21:15:32.279862] Epoch: [9]  [ 900/1011]  eta: 0:00:47  lr: 0.000456  loss: 4.1047 (4.1727)  acc1: 23.4375 (21.3478)  acc5: 47.6562 (45.0910)  time: 0.4205  data: 0.0004  max mem: 37267
[21:16:14.334857] Epoch: [9]  [1000/1011]  eta: 0:00:04  lr: 0.000451  loss: 4.1330 (4.1677)  acc1: 21.0938 (21.4383)  acc5: 45.3125 (45.2286)  time: 0.4198  data: 0.0005  max mem: 37267
[21:16:18.513724] Epoch: [9]  [1010/1011]  eta: 0:00:00  lr: 0.000450  loss: 4.1014 (4.1671)  acc1: 22.2656 (21.4430)  acc5: 46.0938 (45.2433)  time: 0.4184  data: 0.0003  max mem: 37267
[21:16:18.603638] Epoch: [9] Total time: 0:07:09 (0.4244 s / it)
[21:16:18.605651] Averaged stats: lr: 0.000450  loss: 4.1014 (4.1671)  acc1: 22.2656 (21.4430)  acc5: 46.0938 (45.2433)
[21:16:18.606594] * Train_Acc@1 21.443 Acc@5 45.243 loss 4.167
[21:16:20.847915] Test:  [ 0/40]  eta: 0:01:29  loss: 2.9924 (2.9924)  acc1: 31.6406 (31.6406)  acc5: 60.1562 (60.1562)  time: 2.2370  data: 2.0887  max mem: 37267
[21:16:27.879333] Test:  [39/40]  eta: 0:00:00  loss: 2.9568 (2.9793)  acc1: 32.8125 (32.1400)  acc5: 60.1562 (60.0100)  time: 0.1395  data: 0.0377  max mem: 37267
[21:16:27.961977] Test: Total time: 0:00:09 (0.2338 s / it)
[21:16:27.963246] * Acc@1 32.140 Acc@5 60.010 loss 2.979
[21:16:27.963556] Accuracy of the network on the 10000 test images: 32.1%
[21:16:27.963715] Max accuracy: 32.14%
[21:16:27.963843] Saving model at epoch: 9
[21:16:31.483282] Epoch: [10]  [   0/1011]  eta: 0:54:08  lr: 0.000450  loss: 4.1332 (4.1332)  acc1: 24.2188 (24.2188)  acc5: 44.1406 (44.1406)  time: 3.2127  data: 2.7013  max mem: 37267
[21:17:13.892618] Epoch: [10]  [ 100/1011]  eta: 0:06:51  lr: 0.000445  loss: 4.1004 (4.1192)  acc1: 21.8750 (22.8071)  acc5: 46.0938 (45.9700)  time: 0.4210  data: 0.0004  max mem: 37267
[21:17:55.991754] Epoch: [10]  [ 200/1011]  eta: 0:05:53  lr: 0.000439  loss: 4.0749 (4.1115)  acc1: 23.4375 (22.7165)  acc5: 46.8750 (46.3425)  time: 0.4212  data: 0.0004  max mem: 37267
[21:18:38.037953] Epoch: [10]  [ 300/1011]  eta: 0:05:06  lr: 0.000434  loss: 4.1470 (4.1034)  acc1: 21.4844 (22.7666)  acc5: 46.0938 (46.6323)  time: 0.4206  data: 0.0003  max mem: 37267
[21:19:20.187593] Epoch: [10]  [ 400/1011]  eta: 0:04:21  lr: 0.000428  loss: 4.0696 (4.0921)  acc1: 22.2656 (22.9261)  acc5: 48.8281 (47.0435)  time: 0.4206  data: 0.0004  max mem: 37267
[21:20:02.253694] Epoch: [10]  [ 500/1011]  eta: 0:03:38  lr: 0.000423  loss: 4.0099 (4.0826)  acc1: 24.2188 (23.0804)  acc5: 48.0469 (47.3046)  time: 0.4206  data: 0.0004  max mem: 37267
[21:20:44.331067] Epoch: [10]  [ 600/1011]  eta: 0:02:55  lr: 0.000417  loss: 4.0666 (4.0797)  acc1: 21.4844 (23.1307)  acc5: 46.8750 (47.3846)  time: 0.4208  data: 0.0004  max mem: 37267
[21:21:26.398375] Epoch: [10]  [ 700/1011]  eta: 0:02:12  lr: 0.000411  loss: 4.0156 (4.0760)  acc1: 24.2188 (23.2012)  acc5: 48.0469 (47.5036)  time: 0.4205  data: 0.0004  max mem: 37267
[21:22:08.439935] Epoch: [10]  [ 800/1011]  eta: 0:01:29  lr: 0.000405  loss: 3.9651 (4.0703)  acc1: 23.8281 (23.2941)  acc5: 48.8281 (47.6509)  time: 0.4203  data: 0.0004  max mem: 37267
[21:22:50.458098] Epoch: [10]  [ 900/1011]  eta: 0:00:47  lr: 0.000400  loss: 4.0135 (4.0643)  acc1: 24.6094 (23.4124)  acc5: 48.4375 (47.8063)  time: 0.4201  data: 0.0004  max mem: 37267
[21:23:32.448008] Epoch: [10]  [1000/1011]  eta: 0:00:04  lr: 0.000394  loss: 4.0107 (4.0592)  acc1: 24.2188 (23.5444)  acc5: 49.2188 (47.9294)  time: 0.4195  data: 0.0004  max mem: 37267
[21:23:36.627758] Epoch: [10]  [1010/1011]  eta: 0:00:00  lr: 0.000393  loss: 4.0129 (4.0585)  acc1: 23.4375 (23.5534)  acc5: 48.4375 (47.9418)  time: 0.4182  data: 0.0003  max mem: 37267
[21:23:36.719576] Epoch: [10] Total time: 0:07:08 (0.4238 s / it)
[21:23:36.723166] Averaged stats: lr: 0.000393  loss: 4.0129 (4.0585)  acc1: 23.4375 (23.5534)  acc5: 48.4375 (47.9418)
[21:23:36.724845] * Train_Acc@1 23.553 Acc@5 47.942 loss 4.059
[21:23:40.178421] Test:  [ 0/40]  eta: 0:02:17  loss: 2.8039 (2.8039)  acc1: 36.3281 (36.3281)  acc5: 65.6250 (65.6250)  time: 3.4488  data: 3.3000  max mem: 37267
[21:23:45.818705] Test:  [39/40]  eta: 0:00:00  loss: 2.7757 (2.8089)  acc1: 35.1562 (35.7300)  acc5: 64.8438 (64.1100)  time: 0.1574  data: 0.0561  max mem: 37267
[21:23:45.900684] Test: Total time: 0:00:09 (0.2293 s / it)
[21:23:45.903147] * Acc@1 35.730 Acc@5 64.110 loss 2.809
[21:23:45.903808] Accuracy of the network on the 10000 test images: 35.7%
[21:23:45.904177] Max accuracy: 35.73%
[21:23:45.904486] Saving model at epoch: 10
[21:23:48.655910] Epoch: [11]  [   0/1011]  eta: 0:41:17  lr: 0.000393  loss: 3.9073 (3.9073)  acc1: 32.4219 (32.4219)  acc5: 52.3438 (52.3438)  time: 2.4508  data: 1.9196  max mem: 37267
[21:24:31.266258] Epoch: [11]  [ 100/1011]  eta: 0:06:46  lr: 0.000387  loss: 4.0111 (4.0101)  acc1: 23.8281 (24.6403)  acc5: 48.8281 (49.0447)  time: 0.4200  data: 0.0004  max mem: 37267
[21:25:13.268359] Epoch: [11]  [ 200/1011]  eta: 0:05:51  lr: 0.000381  loss: 4.0142 (4.0013)  acc1: 23.4375 (24.8387)  acc5: 49.6094 (49.3975)  time: 0.4200  data: 0.0004  max mem: 37267
[21:25:55.283602] Epoch: [11]  [ 300/1011]  eta: 0:05:04  lr: 0.000375  loss: 3.9686 (3.9935)  acc1: 24.6094 (24.8534)  acc5: 50.3906 (49.5185)  time: 0.4204  data: 0.0005  max mem: 37267
[21:26:37.481419] Epoch: [11]  [ 400/1011]  eta: 0:04:20  lr: 0.000369  loss: 3.9675 (3.9888)  acc1: 25.0000 (24.8812)  acc5: 50.3906 (49.6328)  time: 0.4226  data: 0.0006  max mem: 37267
[21:27:19.753475] Epoch: [11]  [ 500/1011]  eta: 0:03:37  lr: 0.000363  loss: 3.9927 (3.9840)  acc1: 25.0000 (24.9938)  acc5: 49.2188 (49.7466)  time: 0.4226  data: 0.0006  max mem: 37267
[21:28:02.010794] Epoch: [11]  [ 600/1011]  eta: 0:02:54  lr: 0.000357  loss: 3.9895 (3.9809)  acc1: 24.6094 (25.0572)  acc5: 49.2188 (49.8765)  time: 0.4223  data: 0.0005  max mem: 37267
[21:28:44.348702] Epoch: [11]  [ 700/1011]  eta: 0:02:12  lr: 0.000351  loss: 3.9568 (3.9779)  acc1: 25.0000 (25.1755)  acc5: 50.3906 (49.9928)  time: 0.4223  data: 0.0005  max mem: 37267
[21:29:26.575206] Epoch: [11]  [ 800/1011]  eta: 0:01:29  lr: 0.000345  loss: 3.9187 (3.9733)  acc1: 27.3438 (25.3316)  acc5: 50.3906 (50.1219)  time: 0.4221  data: 0.0005  max mem: 37267
[21:30:08.810506] Epoch: [11]  [ 900/1011]  eta: 0:00:47  lr: 0.000339  loss: 3.9231 (3.9688)  acc1: 26.1719 (25.4023)  acc5: 51.9531 (50.2441)  time: 0.4223  data: 0.0006  max mem: 37267
[21:30:51.037772] Epoch: [11]  [1000/1011]  eta: 0:00:04  lr: 0.000332  loss: 3.9042 (3.9644)  acc1: 26.1719 (25.5077)  acc5: 51.1719 (50.3294)  time: 0.4220  data: 0.0005  max mem: 37267
[21:30:55.255479] Epoch: [11]  [1010/1011]  eta: 0:00:00  lr: 0.000332  loss: 3.9146 (3.9640)  acc1: 26.1719 (25.5259)  acc5: 51.5625 (50.3439)  time: 0.4218  data: 0.0003  max mem: 37267
[21:30:55.347846] Epoch: [11] Total time: 0:07:09 (0.4245 s / it)
[21:30:55.349238] Averaged stats: lr: 0.000332  loss: 3.9146 (3.9640)  acc1: 26.1719 (25.5259)  acc5: 51.5625 (50.3439)
[21:30:55.349847] * Train_Acc@1 25.526 Acc@5 50.344 loss 3.964
[21:30:57.525542] Test:  [ 0/40]  eta: 0:01:26  loss: 2.7002 (2.7002)  acc1: 36.3281 (36.3281)  acc5: 67.1875 (67.1875)  time: 2.1722  data: 2.0233  max mem: 37267
[21:31:04.917902] Test:  [39/40]  eta: 0:00:00  loss: 2.7082 (2.7263)  acc1: 36.3281 (36.9000)  acc5: 65.2344 (65.6600)  time: 0.1718  data: 0.0696  max mem: 37267
[21:31:05.003977] Test: Total time: 0:00:09 (0.2413 s / it)
[21:31:05.005503] * Acc@1 36.900 Acc@5 65.660 loss 2.726
[21:31:05.005909] Accuracy of the network on the 10000 test images: 36.9%
[21:31:05.006105] Max accuracy: 36.90%
[21:31:05.006272] Saving model at epoch: 11
[21:31:09.212600] Epoch: [12]  [   0/1011]  eta: 1:06:24  lr: 0.000332  loss: 3.9384 (3.9384)  acc1: 27.3438 (27.3438)  acc5: 50.0000 (50.0000)  time: 3.9412  data: 3.4419  max mem: 37267
[21:31:51.385175] Epoch: [12]  [ 100/1011]  eta: 0:06:55  lr: 0.000326  loss: 3.9021 (3.9208)  acc1: 26.5625 (26.4117)  acc5: 52.7344 (51.2570)  time: 0.4217  data: 0.0005  max mem: 37267
[21:32:33.592864] Epoch: [12]  [ 200/1011]  eta: 0:05:56  lr: 0.000319  loss: 3.9125 (3.9090)  acc1: 26.1719 (26.6363)  acc5: 51.9531 (51.7432)  time: 0.4223  data: 0.0005  max mem: 37267
[21:33:15.776228] Epoch: [12]  [ 300/1011]  eta: 0:05:08  lr: 0.000313  loss: 3.8921 (3.9021)  acc1: 26.9531 (26.7066)  acc5: 51.5625 (51.9077)  time: 0.4220  data: 0.0005  max mem: 37267
[21:33:57.994792] Epoch: [12]  [ 400/1011]  eta: 0:04:23  lr: 0.000307  loss: 3.8936 (3.9013)  acc1: 25.0000 (26.6765)  acc5: 51.9531 (51.9590)  time: 0.4220  data: 0.0005  max mem: 37267
[21:34:40.212523] Epoch: [12]  [ 500/1011]  eta: 0:03:39  lr: 0.000301  loss: 3.8808 (3.8989)  acc1: 26.5625 (26.7278)  acc5: 52.3438 (51.9867)  time: 0.4220  data: 0.0005  max mem: 37267
[21:35:22.427080] Epoch: [12]  [ 600/1011]  eta: 0:02:55  lr: 0.000295  loss: 3.8534 (3.8942)  acc1: 27.3438 (26.8407)  acc5: 51.9531 (52.0584)  time: 0.4222  data: 0.0005  max mem: 37267
[21:36:04.637763] Epoch: [12]  [ 700/1011]  eta: 0:02:12  lr: 0.000288  loss: 3.8235 (3.8889)  acc1: 27.3438 (26.9866)  acc5: 53.5156 (52.2067)  time: 0.4220  data: 0.0005  max mem: 37267
[21:36:46.841564] Epoch: [12]  [ 800/1011]  eta: 0:01:29  lr: 0.000282  loss: 3.8677 (3.8881)  acc1: 26.1719 (26.9765)  acc5: 52.3438 (52.2301)  time: 0.4219  data: 0.0006  max mem: 37267
[21:37:29.162120] Epoch: [12]  [ 900/1011]  eta: 0:00:47  lr: 0.000276  loss: 3.8331 (3.8871)  acc1: 27.7344 (27.0012)  acc5: 52.3438 (52.2653)  time: 0.4220  data: 0.0005  max mem: 37267
[21:38:11.287957] Epoch: [12]  [1000/1011]  eta: 0:00:04  lr: 0.000270  loss: 3.8477 (3.8847)  acc1: 27.3438 (27.0624)  acc5: 53.5156 (52.3519)  time: 0.4203  data: 0.0004  max mem: 37267
[21:38:15.473810] Epoch: [12]  [1010/1011]  eta: 0:00:00  lr: 0.000269  loss: 3.8236 (3.8838)  acc1: 27.7344 (27.0853)  acc5: 52.3438 (52.3689)  time: 0.4190  data: 0.0003  max mem: 37267
[21:38:15.563353] Epoch: [12] Total time: 0:07:10 (0.4256 s / it)
[21:38:15.565448] Averaged stats: lr: 0.000269  loss: 3.8236 (3.8838)  acc1: 27.7344 (27.0853)  acc5: 52.3438 (52.3689)
[21:38:15.566389] * Train_Acc@1 27.085 Acc@5 52.369 loss 3.884
[21:38:17.792012] Test:  [ 0/40]  eta: 0:01:28  loss: 2.5983 (2.5983)  acc1: 41.7969 (41.7969)  acc5: 69.5312 (69.5312)  time: 2.2203  data: 2.0718  max mem: 37267
[21:38:25.711445] Test:  [39/40]  eta: 0:00:00  loss: 2.6001 (2.6172)  acc1: 38.6719 (39.4700)  acc5: 67.1875 (67.6800)  time: 0.1776  data: 0.0763  max mem: 37267
[21:38:25.783428] Test: Total time: 0:00:10 (0.2553 s / it)
[21:38:25.784646] * Acc@1 39.470 Acc@5 67.680 loss 2.617
[21:38:25.784954] Accuracy of the network on the 10000 test images: 39.5%
[21:38:25.785110] Max accuracy: 39.47%
[21:38:25.785223] Saving model at epoch: 12
[21:38:28.536587] Epoch: [13]  [   0/1011]  eta: 0:39:43  lr: 0.000269  loss: 3.6828 (3.6828)  acc1: 27.7344 (27.7344)  acc5: 56.6406 (56.6406)  time: 2.3575  data: 1.7750  max mem: 37267
[21:39:11.549007] Epoch: [13]  [ 100/1011]  eta: 0:06:49  lr: 0.000263  loss: 3.7966 (3.8390)  acc1: 28.1250 (28.1057)  acc5: 54.6875 (53.5234)  time: 0.4214  data: 0.0004  max mem: 37267
[21:39:53.677177] Epoch: [13]  [ 200/1011]  eta: 0:05:53  lr: 0.000257  loss: 3.7869 (3.8445)  acc1: 28.5156 (27.8432)  acc5: 54.6875 (53.2708)  time: 0.4210  data: 0.0004  max mem: 37267
[21:40:35.764176] Epoch: [13]  [ 300/1011]  eta: 0:05:06  lr: 0.000251  loss: 3.7981 (3.8457)  acc1: 27.3438 (27.9031)  acc5: 54.2969 (53.2833)  time: 0.4208  data: 0.0004  max mem: 37267
[21:41:17.865937] Epoch: [13]  [ 400/1011]  eta: 0:04:21  lr: 0.000245  loss: 3.8338 (3.8399)  acc1: 28.1250 (28.0286)  acc5: 52.3438 (53.3695)  time: 0.4211  data: 0.0004  max mem: 37267
[21:42:00.000859] Epoch: [13]  [ 500/1011]  eta: 0:03:38  lr: 0.000239  loss: 3.7700 (3.8370)  acc1: 28.9062 (28.0923)  acc5: 53.9062 (53.5047)  time: 0.4212  data: 0.0004  max mem: 37267
[21:42:42.140314] Epoch: [13]  [ 600/1011]  eta: 0:02:55  lr: 0.000233  loss: 3.7989 (3.8336)  acc1: 28.9062 (28.1705)  acc5: 53.9062 (53.6125)  time: 0.4213  data: 0.0005  max mem: 37267
[21:43:24.265045] Epoch: [13]  [ 700/1011]  eta: 0:02:12  lr: 0.000226  loss: 3.8190 (3.8290)  acc1: 28.1250 (28.2626)  acc5: 54.2969 (53.7157)  time: 0.4213  data: 0.0004  max mem: 37267
[21:44:06.399352] Epoch: [13]  [ 800/1011]  eta: 0:01:29  lr: 0.000220  loss: 3.7860 (3.8277)  acc1: 28.5156 (28.2952)  acc5: 53.9062 (53.7482)  time: 0.4219  data: 0.0004  max mem: 37267
[21:44:48.516735] Epoch: [13]  [ 900/1011]  eta: 0:00:47  lr: 0.000215  loss: 3.7988 (3.8246)  acc1: 28.5156 (28.3821)  acc5: 54.6875 (53.8377)  time: 0.4211  data: 0.0004  max mem: 37267
[21:45:30.626194] Epoch: [13]  [1000/1011]  eta: 0:00:04  lr: 0.000209  loss: 3.7400 (3.8206)  acc1: 30.4688 (28.4848)  acc5: 55.4688 (53.9312)  time: 0.4205  data: 0.0005  max mem: 37267
[21:45:34.819715] Epoch: [13]  [1010/1011]  eta: 0:00:00  lr: 0.000208  loss: 3.7523 (3.8204)  acc1: 29.2969 (28.4785)  acc5: 55.4688 (53.9318)  time: 0.4195  data: 0.0003  max mem: 37267
[21:45:34.903203] Epoch: [13] Total time: 0:07:08 (0.4241 s / it)
[21:45:34.904827] Averaged stats: lr: 0.000208  loss: 3.7523 (3.8204)  acc1: 29.2969 (28.4785)  acc5: 55.4688 (53.9318)
[21:45:34.905561] * Train_Acc@1 28.479 Acc@5 53.932 loss 3.820
[21:45:37.641603] Test:  [ 0/40]  eta: 0:01:49  loss: 2.5606 (2.5606)  acc1: 40.6250 (40.6250)  acc5: 70.3125 (70.3125)  time: 2.7318  data: 2.5834  max mem: 37267
[21:45:44.851480] Test:  [39/40]  eta: 0:00:00  loss: 2.5080 (2.5502)  acc1: 41.0156 (40.9200)  acc5: 69.5312 (69.2500)  time: 0.1786  data: 0.0772  max mem: 37267
[21:45:44.930241] Test: Total time: 0:00:10 (0.2505 s / it)
[21:45:44.932674] * Acc@1 40.920 Acc@5 69.250 loss 2.550
[21:45:44.933325] Accuracy of the network on the 10000 test images: 40.9%
[21:45:44.933697] Max accuracy: 40.92%
[21:45:44.934010] Saving model at epoch: 13
[21:45:47.873182] Epoch: [14]  [   0/1011]  eta: 0:44:35  lr: 0.000208  loss: 3.8266 (3.8266)  acc1: 26.5625 (26.5625)  acc5: 55.8594 (55.8594)  time: 2.6466  data: 2.1353  max mem: 37267
[21:46:30.420281] Epoch: [14]  [ 100/1011]  eta: 0:06:47  lr: 0.000202  loss: 3.7398 (3.7778)  acc1: 28.5156 (29.1460)  acc5: 54.6875 (54.7378)  time: 0.4212  data: 0.0004  max mem: 37267
[21:47:12.531099] Epoch: [14]  [ 200/1011]  eta: 0:05:52  lr: 0.000196  loss: 3.7486 (3.7747)  acc1: 28.9062 (29.2561)  acc5: 55.8594 (54.9207)  time: 0.4211  data: 0.0004  max mem: 37267
[21:47:54.639645] Epoch: [14]  [ 300/1011]  eta: 0:05:05  lr: 0.000190  loss: 3.7292 (3.7719)  acc1: 30.0781 (29.2891)  acc5: 56.2500 (55.0379)  time: 0.4195  data: 0.0004  max mem: 37267
[21:48:36.782815] Epoch: [14]  [ 400/1011]  eta: 0:04:21  lr: 0.000185  loss: 3.7858 (3.7720)  acc1: 27.7344 (29.3417)  acc5: 55.0781 (55.0255)  time: 0.4213  data: 0.0004  max mem: 37267
[21:49:18.923969] Epoch: [14]  [ 500/1011]  eta: 0:03:37  lr: 0.000179  loss: 3.7329 (3.7741)  acc1: 30.4688 (29.3631)  acc5: 55.8594 (54.9378)  time: 0.4214  data: 0.0005  max mem: 37267
[21:50:01.055161] Epoch: [14]  [ 600/1011]  eta: 0:02:54  lr: 0.000173  loss: 3.8206 (3.7727)  acc1: 27.7344 (29.3625)  acc5: 54.6875 (55.0229)  time: 0.4214  data: 0.0005  max mem: 37267
[21:50:43.184578] Epoch: [14]  [ 700/1011]  eta: 0:02:12  lr: 0.000168  loss: 3.7308 (3.7706)  acc1: 29.6875 (29.4484)  acc5: 56.2500 (55.0765)  time: 0.4209  data: 0.0004  max mem: 37267
[21:51:25.297650] Epoch: [14]  [ 800/1011]  eta: 0:01:29  lr: 0.000162  loss: 3.7210 (3.7690)  acc1: 31.2500 (29.5363)  acc5: 54.6875 (55.1113)  time: 0.4212  data: 0.0005  max mem: 37267
[21:52:07.417674] Epoch: [14]  [ 900/1011]  eta: 0:00:47  lr: 0.000157  loss: 3.7285 (3.7670)  acc1: 30.0781 (29.5991)  acc5: 55.8594 (55.1618)  time: 0.4210  data: 0.0004  max mem: 37267
[21:52:49.527447] Epoch: [14]  [1000/1011]  eta: 0:00:04  lr: 0.000151  loss: 3.7318 (3.7646)  acc1: 29.2969 (29.6192)  acc5: 55.8594 (55.2311)  time: 0.4205  data: 0.0005  max mem: 37267
[21:52:53.719940] Epoch: [14]  [1010/1011]  eta: 0:00:00  lr: 0.000151  loss: 3.7260 (3.7645)  acc1: 29.2969 (29.6195)  acc5: 55.8594 (55.2400)  time: 0.4195  data: 0.0003  max mem: 37267
[21:52:53.805879] Epoch: [14] Total time: 0:07:08 (0.4239 s / it)
[21:52:53.808344] Averaged stats: lr: 0.000151  loss: 3.7260 (3.7645)  acc1: 29.2969 (29.6195)  acc5: 55.8594 (55.2400)
[21:52:53.809711] * Train_Acc@1 29.619 Acc@5 55.240 loss 3.764
[21:52:55.893526] Test:  [ 0/40]  eta: 0:01:23  loss: 2.5199 (2.5199)  acc1: 39.8438 (39.8438)  acc5: 69.5312 (69.5312)  time: 2.0780  data: 1.9261  max mem: 37267
[21:53:03.211253] Test:  [39/40]  eta: 0:00:00  loss: 2.4699 (2.4843)  acc1: 41.4062 (42.0000)  acc5: 69.9219 (70.7200)  time: 0.1323  data: 0.0314  max mem: 37267
[21:53:03.293029] Test: Total time: 0:00:09 (0.2370 s / it)
[21:53:03.294538] * Acc@1 42.000 Acc@5 70.720 loss 2.484
[21:53:03.295001] Accuracy of the network on the 10000 test images: 42.0%
[21:53:03.295266] Max accuracy: 42.00%
[21:53:03.295486] Saving model at epoch: 14
[21:53:07.684950] Epoch: [15]  [   0/1011]  eta: 1:06:17  lr: 0.000151  loss: 3.7052 (3.7052)  acc1: 32.0312 (32.0312)  acc5: 53.1250 (53.1250)  time: 3.9347  data: 3.4380  max mem: 37267
[21:53:49.751816] Epoch: [15]  [ 100/1011]  eta: 0:06:54  lr: 0.000145  loss: 3.7118 (3.7265)  acc1: 30.0781 (30.3373)  acc5: 55.8594 (56.2732)  time: 0.4210  data: 0.0004  max mem: 37267
[21:54:31.859560] Epoch: [15]  [ 200/1011]  eta: 0:05:55  lr: 0.000140  loss: 3.7500 (3.7366)  acc1: 30.4688 (30.3152)  acc5: 55.0781 (56.0110)  time: 0.4213  data: 0.0004  max mem: 37267
[21:55:14.090850] Epoch: [15]  [ 300/1011]  eta: 0:05:07  lr: 0.000135  loss: 3.6716 (3.7297)  acc1: 30.8594 (30.4688)  acc5: 57.8125 (56.1085)  time: 0.4215  data: 0.0004  max mem: 37267
[21:55:56.224782] Epoch: [15]  [ 400/1011]  eta: 0:04:22  lr: 0.000130  loss: 3.8081 (3.7314)  acc1: 28.9062 (30.4502)  acc5: 52.3438 (56.0221)  time: 0.4213  data: 0.0005  max mem: 37267
[21:56:38.373251] Epoch: [15]  [ 500/1011]  eta: 0:03:38  lr: 0.000125  loss: 3.7905 (3.7324)  acc1: 29.6875 (30.3884)  acc5: 54.2969 (55.9420)  time: 0.4215  data: 0.0004  max mem: 37267
[21:57:20.501961] Epoch: [15]  [ 600/1011]  eta: 0:02:55  lr: 0.000120  loss: 3.7160 (3.7315)  acc1: 30.4688 (30.4122)  acc5: 55.4688 (55.9471)  time: 0.4216  data: 0.0005  max mem: 37267
[21:58:02.646785] Epoch: [15]  [ 700/1011]  eta: 0:02:12  lr: 0.000115  loss: 3.7475 (3.7289)  acc1: 29.6875 (30.4632)  acc5: 55.4688 (56.0293)  time: 0.4218  data: 0.0005  max mem: 37267
[21:58:44.773137] Epoch: [15]  [ 800/1011]  eta: 0:01:29  lr: 0.000110  loss: 3.7103 (3.7286)  acc1: 30.0781 (30.4819)  acc5: 56.6406 (56.0413)  time: 0.4210  data: 0.0004  max mem: 37267
[21:59:26.890378] Epoch: [15]  [ 900/1011]  eta: 0:00:47  lr: 0.000105  loss: 3.7200 (3.7244)  acc1: 30.4688 (30.5260)  acc5: 55.4688 (56.1230)  time: 0.4214  data: 0.0004  max mem: 37267
[22:00:09.004240] Epoch: [15]  [1000/1011]  eta: 0:00:04  lr: 0.000101  loss: 3.6993 (3.7221)  acc1: 29.2969 (30.5249)  acc5: 56.6406 (56.1919)  time: 0.4207  data: 0.0005  max mem: 37267
[22:00:13.200151] Epoch: [15]  [1010/1011]  eta: 0:00:00  lr: 0.000100  loss: 3.6993 (3.7218)  acc1: 30.0781 (30.5279)  acc5: 56.6406 (56.2009)  time: 0.4197  data: 0.0003  max mem: 37267
[22:00:13.294514] Epoch: [15] Total time: 0:07:09 (0.4249 s / it)
[22:00:13.297027] Averaged stats: lr: 0.000100  loss: 3.6993 (3.7218)  acc1: 30.0781 (30.5279)  acc5: 56.6406 (56.2009)
[22:00:13.298320] * Train_Acc@1 30.528 Acc@5 56.201 loss 3.722
[22:00:15.449233] Test:  [ 0/40]  eta: 0:01:25  loss: 2.4494 (2.4494)  acc1: 44.5312 (44.5312)  acc5: 73.8281 (73.8281)  time: 2.1450  data: 2.0004  max mem: 37267
[22:00:22.542367] Test:  [39/40]  eta: 0:00:00  loss: 2.4122 (2.4414)  acc1: 43.7500 (43.5500)  acc5: 70.7031 (71.3100)  time: 0.1475  data: 0.0456  max mem: 37267
[22:00:22.632873] Test: Total time: 0:00:09 (0.2332 s / it)
[22:00:22.635895] * Acc@1 43.550 Acc@5 71.310 loss 2.441
[22:00:22.636589] Accuracy of the network on the 10000 test images: 43.5%
[22:00:22.636968] Max accuracy: 43.55%
[22:00:22.637285] Saving model at epoch: 15
[22:00:26.950330] Epoch: [16]  [   0/1011]  eta: 1:07:40  lr: 0.000100  loss: 3.7402 (3.7402)  acc1: 27.3438 (27.3438)  acc5: 57.4219 (57.4219)  time: 4.0162  data: 3.5127  max mem: 37267
[22:01:09.033749] Epoch: [16]  [ 100/1011]  eta: 0:06:55  lr: 0.000096  loss: 3.6582 (3.7091)  acc1: 30.0781 (30.8130)  acc5: 57.4219 (56.4434)  time: 0.4210  data: 0.0004  max mem: 37267
[22:01:51.116532] Epoch: [16]  [ 200/1011]  eta: 0:05:55  lr: 0.000091  loss: 3.6924 (3.7069)  acc1: 30.4688 (30.8555)  acc5: 56.6406 (56.5979)  time: 0.4210  data: 0.0004  max mem: 37267
[22:02:33.228108] Epoch: [16]  [ 300/1011]  eta: 0:05:07  lr: 0.000087  loss: 3.7124 (3.7066)  acc1: 30.8594 (30.9022)  acc5: 55.4688 (56.6263)  time: 0.4209  data: 0.0004  max mem: 37267
[22:03:15.330338] Epoch: [16]  [ 400/1011]  eta: 0:04:22  lr: 0.000082  loss: 3.6973 (3.7002)  acc1: 30.0781 (31.0240)  acc5: 57.8125 (56.9173)  time: 0.4212  data: 0.0004  max mem: 37267
[22:03:57.545874] Epoch: [16]  [ 500/1011]  eta: 0:03:38  lr: 0.000078  loss: 3.6723 (3.6969)  acc1: 31.2500 (31.1206)  acc5: 55.8594 (56.9556)  time: 0.4210  data: 0.0005  max mem: 37267
[22:04:39.662308] Epoch: [16]  [ 600/1011]  eta: 0:02:55  lr: 0.000074  loss: 3.7044 (3.6963)  acc1: 30.0781 (31.1194)  acc5: 54.6875 (56.9630)  time: 0.4210  data: 0.0005  max mem: 37267
[22:05:21.802002] Epoch: [16]  [ 700/1011]  eta: 0:02:12  lr: 0.000070  loss: 3.6650 (3.6951)  acc1: 33.5938 (31.1670)  acc5: 58.5938 (56.9772)  time: 0.4213  data: 0.0004  max mem: 37267
[22:06:03.924964] Epoch: [16]  [ 800/1011]  eta: 0:01:29  lr: 0.000066  loss: 3.6411 (3.6922)  acc1: 30.0781 (31.2349)  acc5: 57.8125 (57.0132)  time: 0.4216  data: 0.0005  max mem: 37267
[22:06:46.034293] Epoch: [16]  [ 900/1011]  eta: 0:00:47  lr: 0.000062  loss: 3.6896 (3.6901)  acc1: 32.0312 (31.2808)  acc5: 55.4688 (57.0226)  time: 0.4212  data: 0.0004  max mem: 37267
[22:07:28.139555] Epoch: [16]  [1000/1011]  eta: 0:00:04  lr: 0.000059  loss: 3.6880 (3.6890)  acc1: 30.8594 (31.3265)  acc5: 56.6406 (57.0539)  time: 0.4203  data: 0.0004  max mem: 37267
[22:07:32.329601] Epoch: [16]  [1010/1011]  eta: 0:00:00  lr: 0.000058  loss: 3.6540 (3.6889)  acc1: 31.2500 (31.3250)  acc5: 57.0312 (57.0490)  time: 0.4193  data: 0.0003  max mem: 37267
[22:07:32.424098] Epoch: [16] Total time: 0:07:09 (0.4248 s / it)
[22:07:32.425378] Averaged stats: lr: 0.000058  loss: 3.6540 (3.6889)  acc1: 31.2500 (31.3250)  acc5: 57.0312 (57.0490)
[22:07:32.425987] * Train_Acc@1 31.325 Acc@5 57.049 loss 3.689
[22:07:34.968818] Test:  [ 0/40]  eta: 0:01:41  loss: 2.4164 (2.4164)  acc1: 46.0938 (46.0938)  acc5: 73.4375 (73.4375)  time: 2.5395  data: 2.3913  max mem: 37267
[22:07:42.102394] Test:  [39/40]  eta: 0:00:00  loss: 2.4133 (2.4058)  acc1: 42.9688 (43.9800)  acc5: 72.2656 (71.8700)  time: 0.1522  data: 0.0508  max mem: 37267
[22:07:42.180019] Test: Total time: 0:00:09 (0.2438 s / it)
[22:07:42.182529] * Acc@1 43.980 Acc@5 71.870 loss 2.406
[22:07:42.183206] Accuracy of the network on the 10000 test images: 44.0%
[22:07:42.183581] Max accuracy: 43.98%
[22:07:42.183918] Saving model at epoch: 16
[22:07:46.352306] Epoch: [17]  [   0/1011]  eta: 1:05:25  lr: 0.000058  loss: 3.7212 (3.7212)  acc1: 29.2969 (29.2969)  acc5: 55.4688 (55.4688)  time: 3.8830  data: 3.4030  max mem: 37267
[22:08:28.433656] Epoch: [17]  [ 100/1011]  eta: 0:06:54  lr: 0.000055  loss: 3.6816 (3.6795)  acc1: 31.6406 (31.4472)  acc5: 57.0312 (57.2014)  time: 0.4213  data: 0.0004  max mem: 37267
[22:09:10.534260] Epoch: [17]  [ 200/1011]  eta: 0:05:55  lr: 0.000051  loss: 3.6911 (3.6806)  acc1: 31.2500 (31.4132)  acc5: 57.8125 (57.1692)  time: 0.4208  data: 0.0004  max mem: 37267
[22:09:52.665118] Epoch: [17]  [ 300/1011]  eta: 0:05:07  lr: 0.000048  loss: 3.6508 (3.6750)  acc1: 32.0312 (31.5368)  acc5: 57.4219 (57.3077)  time: 0.4211  data: 0.0004  max mem: 37267
[22:10:34.783515] Epoch: [17]  [ 400/1011]  eta: 0:04:22  lr: 0.000044  loss: 3.7077 (3.6793)  acc1: 30.8594 (31.4906)  acc5: 56.2500 (57.1900)  time: 0.4212  data: 0.0004  max mem: 37267
[22:11:16.906596] Epoch: [17]  [ 500/1011]  eta: 0:03:38  lr: 0.000041  loss: 3.6568 (3.6761)  acc1: 31.6406 (31.5494)  acc5: 57.8125 (57.3151)  time: 0.4213  data: 0.0004  max mem: 37267
[22:11:59.025485] Epoch: [17]  [ 600/1011]  eta: 0:02:55  lr: 0.000038  loss: 3.6117 (3.6733)  acc1: 32.4219 (31.5776)  acc5: 59.3750 (57.3770)  time: 0.4211  data: 0.0004  max mem: 37267
[22:12:41.265965] Epoch: [17]  [ 700/1011]  eta: 0:02:12  lr: 0.000035  loss: 3.6568 (3.6725)  acc1: 32.8125 (31.6228)  acc5: 57.4219 (57.3996)  time: 0.4214  data: 0.0005  max mem: 37267
[22:13:23.394931] Epoch: [17]  [ 800/1011]  eta: 0:01:29  lr: 0.000032  loss: 3.6667 (3.6713)  acc1: 31.2500 (31.6284)  acc5: 55.4688 (57.3951)  time: 0.4212  data: 0.0004  max mem: 37267
[22:14:05.502627] Epoch: [17]  [ 900/1011]  eta: 0:00:47  lr: 0.000030  loss: 3.6604 (3.6707)  acc1: 32.8125 (31.6424)  acc5: 55.8594 (57.3907)  time: 0.4211  data: 0.0004  max mem: 37267
[22:14:47.611259] Epoch: [17]  [1000/1011]  eta: 0:00:04  lr: 0.000027  loss: 3.6624 (3.6694)  acc1: 31.6406 (31.6925)  acc5: 57.4219 (57.4149)  time: 0.4205  data: 0.0004  max mem: 37267
[22:14:51.801435] Epoch: [17]  [1010/1011]  eta: 0:00:00  lr: 0.000027  loss: 3.6333 (3.6687)  acc1: 31.6406 (31.7048)  acc5: 58.9844 (57.4304)  time: 0.4194  data: 0.0003  max mem: 37267
[22:14:51.895860] Epoch: [17] Total time: 0:07:09 (0.4248 s / it)
[22:14:51.897270] Averaged stats: lr: 0.000027  loss: 3.6333 (3.6687)  acc1: 31.6406 (31.7048)  acc5: 58.9844 (57.4304)
[22:14:51.897865] * Train_Acc@1 31.705 Acc@5 57.430 loss 3.669
[22:14:54.079943] Test:  [ 0/40]  eta: 0:01:27  loss: 2.4608 (2.4608)  acc1: 46.0938 (46.0938)  acc5: 71.8750 (71.8750)  time: 2.1786  data: 2.0302  max mem: 37267
[22:15:01.803635] Test:  [39/40]  eta: 0:00:00  loss: 2.3874 (2.3913)  acc1: 44.5312 (44.4500)  acc5: 71.0938 (71.8900)  time: 0.1735  data: 0.0725  max mem: 37267
[22:15:01.878286] Test: Total time: 0:00:09 (0.2494 s / it)
[22:15:01.880338] * Acc@1 44.450 Acc@5 71.890 loss 2.391
[22:15:01.881006] Accuracy of the network on the 10000 test images: 44.5%
[22:15:01.881371] Max accuracy: 44.45%
[22:15:01.881699] Saving model at epoch: 17
[22:15:06.081948] Epoch: [18]  [   0/1011]  eta: 1:05:58  lr: 0.000027  loss: 3.5524 (3.5524)  acc1: 33.5938 (33.5938)  acc5: 59.7656 (59.7656)  time: 3.9153  data: 3.4200  max mem: 37267
[22:15:48.175090] Epoch: [18]  [ 100/1011]  eta: 0:06:54  lr: 0.000024  loss: 3.6401 (3.6621)  acc1: 31.2500 (31.6870)  acc5: 57.8125 (57.4025)  time: 0.4212  data: 0.0004  max mem: 37267
[22:16:30.310121] Epoch: [18]  [ 200/1011]  eta: 0:05:55  lr: 0.000022  loss: 3.7034 (3.6618)  acc1: 32.0312 (31.9593)  acc5: 56.2500 (57.4607)  time: 0.4213  data: 0.0004  max mem: 37267
[22:17:12.458700] Epoch: [18]  [ 300/1011]  eta: 0:05:07  lr: 0.000020  loss: 3.6200 (3.6598)  acc1: 32.8125 (31.9754)  acc5: 57.4219 (57.6178)  time: 0.4212  data: 0.0005  max mem: 37267
[22:17:54.573903] Epoch: [18]  [ 400/1011]  eta: 0:04:22  lr: 0.000018  loss: 3.6428 (3.6556)  acc1: 31.6406 (32.0264)  acc5: 57.4219 (57.7151)  time: 0.4211  data: 0.0004  max mem: 37267
[22:18:36.716547] Epoch: [18]  [ 500/1011]  eta: 0:03:38  lr: 0.000016  loss: 3.7331 (3.6560)  acc1: 30.0781 (32.0445)  acc5: 55.8594 (57.6511)  time: 0.4212  data: 0.0004  max mem: 37267
[22:19:18.815869] Epoch: [18]  [ 600/1011]  eta: 0:02:55  lr: 0.000014  loss: 3.6389 (3.6563)  acc1: 31.6406 (32.0144)  acc5: 57.4219 (57.7222)  time: 0.4212  data: 0.0004  max mem: 37267
[22:20:00.934340] Epoch: [18]  [ 700/1011]  eta: 0:02:12  lr: 0.000012  loss: 3.6826 (3.6599)  acc1: 30.4688 (31.9337)  acc5: 56.6406 (57.6626)  time: 0.4210  data: 0.0004  max mem: 37267
[22:20:43.060449] Epoch: [18]  [ 800/1011]  eta: 0:01:29  lr: 0.000011  loss: 3.6449 (3.6604)  acc1: 32.4219 (31.9430)  acc5: 58.2031 (57.6545)  time: 0.4209  data: 0.0004  max mem: 37267
[22:21:25.200213] Epoch: [18]  [ 900/1011]  eta: 0:00:47  lr: 0.000009  loss: 3.6463 (3.6612)  acc1: 30.4688 (31.9034)  acc5: 57.4219 (57.6512)  time: 0.4214  data: 0.0005  max mem: 37267
[22:22:07.393549] Epoch: [18]  [1000/1011]  eta: 0:00:04  lr: 0.000008  loss: 3.6689 (3.6606)  acc1: 30.8594 (31.9150)  acc5: 55.8594 (57.6630)  time: 0.4202  data: 0.0005  max mem: 37267
[22:22:11.584157] Epoch: [18]  [1010/1011]  eta: 0:00:00  lr: 0.000008  loss: 3.6448 (3.6598)  acc1: 32.4219 (31.9362)  acc5: 58.2031 (57.6777)  time: 0.4192  data: 0.0003  max mem: 37267
[22:22:11.673440] Epoch: [18] Total time: 0:07:09 (0.4248 s / it)
[22:22:11.674858] Averaged stats: lr: 0.000008  loss: 3.6448 (3.6598)  acc1: 32.4219 (31.9362)  acc5: 58.2031 (57.6777)
[22:22:11.675446] * Train_Acc@1 31.936 Acc@5 57.678 loss 3.660
[22:22:13.768439] Test:  [ 0/40]  eta: 0:01:23  loss: 2.4116 (2.4116)  acc1: 43.3594 (43.3594)  acc5: 73.8281 (73.8281)  time: 2.0896  data: 1.9407  max mem: 37267
[22:22:21.230424] Test:  [39/40]  eta: 0:00:00  loss: 2.3575 (2.3840)  acc1: 44.5312 (44.4900)  acc5: 73.0469 (72.4000)  time: 0.1487  data: 0.0476  max mem: 37267
[22:22:21.309146] Test: Total time: 0:00:09 (0.2408 s / it)
[22:22:21.310594] * Acc@1 44.490 Acc@5 72.400 loss 2.384
[22:22:21.310905] Accuracy of the network on the 10000 test images: 44.5%
[22:22:21.311057] Max accuracy: 44.49%
[22:22:21.311181] Saving model at epoch: 18
[22:22:24.092501] Epoch: [19]  [   0/1011]  eta: 0:41:35  lr: 0.000008  loss: 3.5296 (3.5296)  acc1: 35.1562 (35.1562)  acc5: 60.9375 (60.9375)  time: 2.4680  data: 1.9323  max mem: 37267
[22:23:06.903453] Epoch: [19]  [ 100/1011]  eta: 0:06:48  lr: 0.000006  loss: 3.6631 (3.6502)  acc1: 32.0312 (32.2401)  acc5: 57.8125 (58.1412)  time: 0.4211  data: 0.0005  max mem: 37267
[22:23:49.031079] Epoch: [19]  [ 200/1011]  eta: 0:05:52  lr: 0.000005  loss: 3.6546 (3.6589)  acc1: 32.0312 (32.0779)  acc5: 57.4219 (57.7270)  time: 0.4213  data: 0.0004  max mem: 37267
[22:24:31.155499] Epoch: [19]  [ 300/1011]  eta: 0:05:05  lr: 0.000004  loss: 3.6178 (3.6539)  acc1: 31.2500 (32.0793)  acc5: 58.5938 (57.9449)  time: 0.4212  data: 0.0005  max mem: 37267
[22:25:13.309073] Epoch: [19]  [ 400/1011]  eta: 0:04:21  lr: 0.000003  loss: 3.6280 (3.6560)  acc1: 32.8125 (32.0527)  acc5: 57.4219 (57.8106)  time: 0.4218  data: 0.0005  max mem: 37267
[22:25:55.438316] Epoch: [19]  [ 500/1011]  eta: 0:03:38  lr: 0.000003  loss: 3.6369 (3.6552)  acc1: 32.8125 (32.0453)  acc5: 58.5938 (57.8258)  time: 0.4215  data: 0.0005  max mem: 37267
[22:26:37.567378] Epoch: [19]  [ 600/1011]  eta: 0:02:55  lr: 0.000002  loss: 3.6556 (3.6558)  acc1: 31.6406 (32.0066)  acc5: 57.0312 (57.8229)  time: 0.4213  data: 0.0005  max mem: 37267
[22:27:19.692345] Epoch: [19]  [ 700/1011]  eta: 0:02:12  lr: 0.000002  loss: 3.6000 (3.6525)  acc1: 32.0312 (32.0847)  acc5: 59.3750 (57.9245)  time: 0.4213  data: 0.0005  max mem: 37267
[22:28:01.833274] Epoch: [19]  [ 800/1011]  eta: 0:01:29  lr: 0.000001  loss: 3.6384 (3.6520)  acc1: 32.4219 (32.1059)  acc5: 59.3750 (57.9490)  time: 0.4213  data: 0.0004  max mem: 37267
[22:28:43.956824] Epoch: [19]  [ 900/1011]  eta: 0:00:47  lr: 0.000001  loss: 3.6436 (3.6517)  acc1: 32.4219 (32.1249)  acc5: 58.2031 (57.9781)  time: 0.4209  data: 0.0005  max mem: 37267
[22:29:26.065773] Epoch: [19]  [1000/1011]  eta: 0:00:04  lr: 0.000001  loss: 3.6612 (3.6530)  acc1: 32.4219 (32.0972)  acc5: 56.2500 (57.9467)  time: 0.4208  data: 0.0004  max mem: 37267
[22:29:30.259423] Epoch: [19]  [1010/1011]  eta: 0:00:00  lr: 0.000001  loss: 3.6413 (3.6532)  acc1: 32.0312 (32.0884)  acc5: 57.8125 (57.9396)  time: 0.4197  data: 0.0003  max mem: 37267
[22:29:30.344893] Epoch: [19] Total time: 0:07:08 (0.4241 s / it)
[22:29:30.346507] Averaged stats: lr: 0.000001  loss: 3.6413 (3.6532)  acc1: 32.0312 (32.0884)  acc5: 57.8125 (57.9396)
[22:29:30.347103] * Train_Acc@1 32.088 Acc@5 57.940 loss 3.653
[22:29:30.348024] Saving model at epoch: 19
[22:29:32.650012] Test:  [ 0/40]  eta: 0:01:20  loss: 2.4121 (2.4121)  acc1: 44.9219 (44.9219)  acc5: 73.0469 (73.0469)  time: 2.0026  data: 1.8541  max mem: 37267
[22:29:40.516564] Test:  [39/40]  eta: 0:00:00  loss: 2.3838 (2.3848)  acc1: 43.7500 (44.2900)  acc5: 72.6562 (72.3600)  time: 0.1460  data: 0.0451  max mem: 37267
[22:29:40.596924] Test: Total time: 0:00:09 (0.2488 s / it)
[22:29:40.598228] * Acc@1 44.290 Acc@5 72.360 loss 2.385
[22:29:40.598549] Accuracy of the network on the 10000 test images: 44.3%
[22:29:40.598712] Max accuracy: 44.49%
[22:29:40.598842] Saving model at epoch: 19
[22:29:40.891231] Training time 2:26:24
